sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
mylasso.lambda.seq = exp(seq(-10, 1, length.out = 100))
cv.out = cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 0.77,
lambda = mylasso.lambda.seq)
best.lam = cv.out$lambda.min
Ytest.pred = exp(predict(cv.out, s = best.lam, newx = as.matrix(test_matrix_df)))
colnames(Ytest.pred)[1]<-"Sale_Price"
ridge_df = data.frame(PID = test.y[1], Sale_Price = Ytest.pred)
write.csv(ridge_df,"data/mysubmission5.txt", row.names = FALSE, quote=FALSE)
pred <- read.csv("data/mysubmission5.txt")
names(test.y)[2] <- "True_Sale_Price"
pred <- merge(pred, test.y, by="PID")
sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
mylasso.lambda.seq = exp(seq(-10, 1, length.out = 100))
cv.out = cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 0.775,
lambda = mylasso.lambda.seq)
best.lam = cv.out$lambda.min
Ytest.pred = exp(predict(cv.out, s = best.lam, newx = as.matrix(test_matrix_df)))
colnames(Ytest.pred)[1]<-"Sale_Price"
ridge_df = data.frame(PID = test.y[1], Sale_Price = Ytest.pred)
write.csv(ridge_df,"data/mysubmission5.txt", row.names = FALSE, quote=FALSE)
pred <- read.csv("data/mysubmission5.txt")
names(test.y)[2] <- "True_Sale_Price"
pred <- merge(pred, test.y, by="PID")
sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
mylasso.lambda.seq = exp(seq(-10, 1, length.out = 100))
cv.out = cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 0.2,
lambda = mylasso.lambda.seq)
best.lam = cv.out$lambda.min
Ytest.pred = exp(predict(cv.out, s = best.lam, newx = as.matrix(test_matrix_df)))
colnames(Ytest.pred)[1]<-"Sale_Price"
ridge_df = data.frame(PID = test.y[1], Sale_Price = Ytest.pred)
write.csv(ridge_df,"data/mysubmission5.txt", row.names = FALSE, quote=FALSE)
pred <- read.csv("data/mysubmission5.txt")
names(test.y)[2] <- "True_Sale_Price"
pred <- merge(pred, test.y, by="PID")
sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cv.out <- cv.glmnet(as.matrix(train_matrix_df), train.y, alpha = 1)
cv.out <- cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 1)
sel.vars <- predict(cv.out, type="nonzero", s = cv.out$lambda.min)$X1
cv.out <- cv.glmnet(as.matrix(train_matrix_df[, sel.vars]), train.y, alpha = 0)
cv.out <- cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 1)
sel.vars <- predict(cv.out, type="nonzero", s = cv.out$lambda.min)$X1
cv.out <- cv.glmnet(as.matrix(train_matrix_df[, sel.vars]), as.matrix(train.y), alpha = 0)
cv.out <- cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 1)
sel.vars <- predict(cv.out, type="nonzero", s = cv.out$lambda.min)$X1
cv.out <- cv.glmnet(as.matrix(as.matrix(train_matrix_df)[, sel.vars]), as.matrix(train.y), alpha = 0)
cv.out <- cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 1)
sel.vars <- predict(cv.out, type="nonzero", s = cv.out$lambda.min)$X1
cv.out <- cv.glmnet(as.matrix(train_matrix_df)[, sel.vars], as.matrix(train.y), alpha = 0)
cv.out <- cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 1)
sel.vars <- predict(cv.out, type="nonzero", s = cv.out$lambda.min)$X1
cv.out <- cv.glmnet(as.matrix(as.matrix(train_matrix_df)[, sel.vars]), as.matrix(train.y), alpha = 0)
print(as.matrix(as.matrix(train_matrix_df)[, sel.vars]))
View(test_matrix_df)
cv.out <- cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 1)
sel.vars <- predict(cv.out, type="nonzero", s = cv.out$lambda.min)$X1
cv.out <- cv.glmnet(as.matrix(as.matrix(train_matrix_df)[ sel.vars ,]), as.matrix(train.y), alpha = 0)
cv.out <- cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 1)
sel.vars <- predict(cv.out, type="nonzero", s = cv.out$lambda.min)$X1
cv.out <- cv.glmnet(as.matrix(as.matrix(train_matrix_df)[, sel.vars]), as.matrix(train.y), alpha = 0)
print(as.matrix(as.matrix(train_matrix_df)[, sel.vars]))
print(as.matrix(as.matrix(train_matrix_df)[ sel.vars ,]))
print(as.matrix(as.matrix(train_matrix_df)[, sel.vars]))
print(sel.vars)
cv.out <- cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 1)
sel.vars <- predict(cv.out, type="nonzero", s = cv.out$lambda.min)$X1
print(sel.vars)
cv.out <- cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 1)
print(cv.out$lambda.min)
cv.out <- cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 1)
sel.vars <- predict(cv.out, type="nonzero", s = cv.out$lambda.min)#$X1
print(sel.vars)
cv.out <- cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 1)
sel.vars <- predict(cv.out, type="nonzero", s = cv.out$lambda.min)#$X1
print(sel.vars)
cv.out <- cv.glmnet(as.matrix(as.matrix(train_matrix_df)[, sel.vars]), as.matrix(train.y), alpha = 0)
cv.out <- cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 1)
sel.vars <- predict(cv.out, type="nonzero", s = cv.out$lambda.min)$s1
print(sel.vars)
cv.out <- cv.glmnet(as.matrix(as.matrix(train_matrix_df)[, sel.vars]), as.matrix(train.y), alpha = 0)
Ytest.pred <- exp(predict(cv.out, s = cv.out$lambda.min, newx = as.matrix(test_matrix_df[, sel.vars])))
colnames(Ytest.pred)[1]<-"Sale_Price"
ridge_df = data.frame(PID = test.y[1], Sale_Price = Ytest.pred)
write.csv(ridge_df,"data/mysubmission6.txt", row.names = FALSE, quote=FALSE)
pred <- read.csv("data/mysubmission6.txt")
names(test.y)[2] <- "True_Sale_Price"
pred <- merge(pred, test.y, by="PID")
sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cv.out <- cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 1)
sel.vars <- predict(cv.out, type="nonzero", s = cv.out$lambda.min)$s1
cv.out <- cv.glmnet(as.matrix(as.matrix(train_matrix_df)[, sel.vars]), as.matrix(train.y), alpha = .2)
Ytest.pred <- exp(predict(cv.out, s = cv.out$lambda.min, newx = as.matrix(test_matrix_df[, sel.vars])))
colnames(Ytest.pred)[1]<-"Sale_Price"
ridge_df = data.frame(PID = test.y[1], Sale_Price = Ytest.pred)
write.csv(ridge_df,"data/mysubmission6.txt", row.names = FALSE, quote=FALSE)
pred <- read.csv("data/mysubmission6.txt")
names(test.y)[2] <- "True_Sale_Price"
pred <- merge(pred, test.y, by="PID")
sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
cv.out <- cv.glmnet(as.matrix(train_matrix_df), as.matrix(train.y), alpha = 1)
sel.vars <- predict(cv.out, type="nonzero", s = cv.out$lambda.min)$s1
cv.out <- cv.glmnet(as.matrix(as.matrix(train_matrix_df)[, sel.vars]), as.matrix(train.y), alpha = .77)
Ytest.pred <- exp(predict(cv.out, s = cv.out$lambda.min, newx = as.matrix(test_matrix_df[, sel.vars])))
colnames(Ytest.pred)[1]<-"Sale_Price"
ridge_df = data.frame(PID = test.y[1], Sale_Price = Ytest.pred)
write.csv(ridge_df,"data/mysubmission6.txt", row.names = FALSE, quote=FALSE)
pred <- read.csv("data/mysubmission6.txt")
names(test.y)[2] <- "True_Sale_Price"
pred <- merge(pred, test.y, by="PID")
sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
library(lubridate)
library(tidyverse)
install.packages("tidyverse")
library(lubridate)
library(tidyverse)
# read raw data and extract date column
train_raw <- readr::read_csv(unz('data/train.csv.zip', 'data/train.csv'))
train_raw <- readr::read_csv(unz('data/train.csv.zip', 'data/train.csv'))
train_raw <- readr::read_csv(unz('data/train.csv.zip', 'train.csv'))
train_raw <- readr::read_csv(unz('train.csv.zip', 'train.csv'))
train_raw <- readr::read_csv(unz('train.csv.zip', 'data/train.csv'))
train_raw <- readr::read_csv(unz('train.csv.zip', 'data/train.csv'))
x = (1:20)/10
print(x)
x = [1, 3, 5]
x = 1, 3, 5
x = c(1, 3, 5)
print(x)
myboosting=function(G, w, x, y){
# G: previous committee
# w: previous weight
# x: covariates
# y: response
# g: current classifier, y=1, if x < split
n = length(x); pos=sample(1:(n-1),1);
g=c(rep(1, pos),rep(-1, n-pos));
err = sum((1-y*g)*w)/2;
alpha=(1/2)*log((1-err)/(err));
G=G+alpha*g
w1 = w*exp(-alpha*y*g); w1 = w1/sum(w1);
list(G=G, w=w,w1=w1, g=sign(alpha)*g, err=min(err,1-err),
split=(x[pos]+x[pos+1])/2, a=alpha)
}
myplot=function(myout, x,y ){
n=length(x); m = sum(y>0);
tmp1=as.vector(t(matrix(c(myout$w,myout$w1), ncol=2)))
tmp2=as.vector(t(matrix(c(x-0.04,x), ncol=2)))
tmp3=as.vector(t(matrix(c(rep(5, n), rep(6, n)),
ncol=2)));
mymax=max(myout$w1, myout$w);
plot(tmp2, tmp1, type="h", lwd=9, col=tmp3, xlim=c(0,2.2),
ylim=c(-mymax/2, mymax+0.05), axes=T,
ylab="weights", xlab="");
points(x[y>0], rep(-mymax/6, m), pch=3)
points(x[y<0], rep(-mymax/6, (n-m)), pch=0)
tmp1=sum(myout$g>0);
points(x[myout$g>0], rep(-mymax/3, tmp1), pch=3)
points(x[myout$g<0], rep(-mymax/3, (n-tmp1)), pch=0)
tmp1=sum(sign(myout$g[1])*sign(myout$g)==1)
tmp2=sum((x[tmp1]+x[tmp1+1])/2)
lines(c(tmp2, tmp2), c(-mymax/2, mymax+0.03), lty=2)
tmp1=sum(myout$G>0);
points(x[myout$G>0], rep(-mymax/2, tmp1), pch=3)
points(x[myout$G<0],rep(-mymax/2, (n-tmp1)),  pch=0)
text(2.15, -mymax/6, "data")
text(2.15, -mymax/3, "current")
text(2.15, -mymax/2,  "overall")
}
#set.seed(100)
x = c(1, 3, 5)
n = length(x);
y= c(-1, 1, -1)
G=rep(0,n); w=rep(1,n)/n;
myout=myboosting(G, w, x, y);
myplot(myout, x,y);
print(myout)
library(ISLR)
dim(Caravan)
attach(Caravan)
library(ISLR)
dim(Caravan)
head(Caravan)
summary(Purchase)
standardized.X=scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
var(standardized.X[,2])
test=1:1000
train.X=standardized.x[-test,]
library(ISLR)
dim(Caravan)
head(Caravan)
summary(Purchase)
standardized.X=scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
var(standardized.X[,2])
test=1:1000
train.X=standardized.x[-test,]
library(ISLR)
dim(Caravan)
head(Caravan)
summary(Purchase)
standardized.X=scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
var(standardized.X[,2])
test=1:1000
train.X=standardized.X[-test,]
test.X=standardized.X[test,]
train.Y=Purchase[-test]
test.Y=Purchase[test]
set.seed(1)
knn.pred=knn(train.X, test.X,train.Y,k=1)
library(ISLR)
library(caret)
dim(Caravan)
head(Caravan)
summary(Purchase)
standardized.X=scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
var(standardized.X[,2])
test=1:1000
train.X=standardized.X[-test,]
test.X=standardized.X[test,]
train.Y=Purchase[-test]
test.Y=Purchase[test]
set.seed(1)
knn.pred=knn(train.X, test.X,train.Y,k=1)
library(ISLR)
library(pROC)
dim(Caravan)
head(Caravan)
summary(Purchase)
standardized.X=scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
var(standardized.X[,2])
test=1:1000
train.X=standardized.X[-test,]
test.X=standardized.X[test,]
train.Y=Purchase[-test]
test.Y=Purchase[test]
set.seed(1)
knn.pred=knn(train.X, test.X,train.Y,k=1)
install.packages("pROC")
library(ISLR)
library(pROC)
dim(Caravan)
head(Caravan)
summary(Purchase)
library(ISLR)
# library(pROC)
dim(Caravan)
head(Caravan)
summary(Purchase)
library(ISLR)
# library(pROC)
dim(Caravan)
head(Caravan)
summary(Purchase)
library(ISLR)
# library(pROC)
dim(Caravan)
head(Caravan)
summary(Purchase)
library(ISLR)
# library(pROC)
dim(Caravan)
head(Caravan)
summary(Purchase)
library(ISLR)
dim(Caravan)
head(Caravan)
summary(Purchase)
library(ISLR)
dim(Caravan)
head(Caravan)
Purchase
test=1:1000
train.X=standardized.X[-test,]
standardized.X=scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
var(standardized.X[,2])
test=1:1000
train.X=standardized.X[-test,]
test.X=standardized.X[test,]
train.Y=Purchase[-test]
setwd("../Desktop/Jupyter Notebooks/project_2/")
source("mymain.R")
library(readr)
library(magrittr)
library(dplyr)
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
# wae: record weighted mean absolute error WMAE
num_folds <- 10
wae <- rep(0, num_folds)
for (t in 1:num_folds) {
# *** THIS IS YOUR PREDICTION FUNCTION ***
test_pred <- mypredict()
#print(test_pred)
# read new data from fold_t
fold_file <- paste0('fold_', t, '.csv')
new_train <- readr::read_csv(fold_file, col_types = cols())
#print(new_train)
# extract predictions matching up to the new data
scoring_tbl <- new_train %>% left_join(test_pred, by = c('Date', 'Store', 'Dept'))
print(scoring_tbl)
# compute WMAE
actuals <- scoring_tbl$Weekly_Sales
preds <- scoring_tbl$Weekly_Pred
preds[is.na(preds)] <- 0
weights <- if_else(scoring_tbl$IsHoliday, 5, 1)
wae[t] <- sum(weights * abs(actuals - preds)) / sum(weights)
# update train data and get ready to predict at (t+1)
train <- train %>% add_row(new_train)
}
print(wae)
mean(wae)
library(glmnet)
library(dplyr)
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
train_pairs <- train[, 1:2] %>% count(Store, Dept) %>% filter(n != 0)
test_pairs <- test[, 1:2] %>% count(Store, Dept) %>% filter(n != 0)
unique_pairs <- intersect(train_pairs[, 1:2], test_pairs[, 1:2])
# pick out the needed training samples, convert to dummy coding, then put them into a list
train_split <- unique_pairs %>%
left_join(train, by = c('Store', 'Dept')) %>%
mutate(Wk = factor(ifelse(lubridate::year(Date) == 2010, lubridate::week(Date) - 1, lubridate::week(Date)), levels = 1:52)) %>%
mutate(Yr = lubridate::year(Date))
train_split = as_tibble(model.matrix(~ Weekly_Sales + Store + Dept + Yr + Wk, train_split)) %>% group_split(Store, Dept)
# do the same for the test set
test_split <- unique_pairs %>%
left_join(test, by = c('Store', 'Dept')) %>%
mutate(Wk = factor(ifelse(lubridate::year(Date) == 2010, lubridate::week(Date) - 1, lubridate::week(Date)), levels = 1:52)) %>%
mutate(Yr = lubridate::year(Date))
test_split = as_tibble(model.matrix(~ Store + Dept + Yr + Wk, test_split)) %>% mutate(Date = test_split$Date) %>% group_split(Store, Dept)
# pre-allocate a list to store the predictions
test_pred <- vector(mode = "list", length = nrow(unique_pairs))
# perform regression for each split, note we used lm.fit instead of lm
for (i in 1:nrow(unique_pairs)) {
tmp_train <- train_split[[i]]
tmp_test <- test_split[[i]]
mycoef <- lm.fit(as.matrix(tmp_train[, -(2:4)]), tmp_train$Weekly_Sales)$coefficients
mycoef[is.na(mycoef)] <- 0
tmp_pred <- mycoef[1] + as.matrix(tmp_test[, 4:55]) %*% mycoef[-1]
test_pred[[i]] <- cbind(tmp_test[, 2:3], Date = tmp_test$Date, Weekly_Pred = tmp_pred[,1])
}
# turn the list into a table at once,
# this is much more efficient then keep concatenating small tables
test_pred <- bind_rows(test_pred)
View(test_split)
View(tmp_train)
View(tmp_train)
View(tmp_pred)
source("mymain.R")
library(readr)
library(magrittr)
library(dplyr)
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
# wae: record weighted mean absolute error WMAE
num_folds <- 10
wae <- rep(0, num_folds)
for (t in 1:num_folds) {
# *** THIS IS YOUR PREDICTION FUNCTION ***
test_pred <- mypredict()
print(test_pred)
# read new data from fold_t
fold_file <- paste0('fold_', t, '.csv')
new_train <- readr::read_csv(fold_file, col_types = cols())
#print(new_train)
# extract predictions matching up to the new data
scoring_tbl <- new_train %>% left_join(test_pred, by = c('Date', 'Store', 'Dept'))
#print(scoring_tbl)
# compute WMAE
actuals <- scoring_tbl$Weekly_Sales
preds <- scoring_tbl$Weekly_Pred
preds[is.na(preds)] <- 0
weights <- if_else(scoring_tbl$IsHoliday, 5, 1)
wae[t] <- sum(weights * abs(actuals - preds)) / sum(weights)
# update train data and get ready to predict at (t+1)
train <- train %>% add_row(new_train)
}
print(wae)
mean(wae)
View(tmp_train)
train_pairs <- train[, 1:2] %>% count(Store, Dept) %>% filter(n != 0)
test_pairs <- test[, 1:2] %>% count(Store, Dept) %>% filter(n != 0)
unique_pairs <- intersect(train_pairs[, 1:2], test_pairs[, 1:2])
# pick out the needed training samples, convert to dummy coding, then put them into a list
train_split <- unique_pairs %>%
left_join(train, by = c('Store', 'Dept')) %>%
mutate(Wk = factor(ifelse(lubridate::year(Date) == 2010, lubridate::week(Date) - 1, lubridate::week(Date)), levels = 1:52)) %>%
mutate(Yr = lubridate::year(Date))
train_split = as_tibble(model.matrix(~ Weekly_Sales + Store + Dept + Yr + Wk, train_split)) %>% group_split(Store, Dept)
# do the same for the test set
test_split <- unique_pairs %>%
left_join(test, by = c('Store', 'Dept')) %>%
mutate(Wk = factor(ifelse(lubridate::year(Date) == 2010, lubridate::week(Date) - 1, lubridate::week(Date)), levels = 1:52)) %>%
mutate(Yr = lubridate::year(Date))
test_split = as_tibble(model.matrix(~ Store + Dept + Yr + Wk, test_split)) %>% mutate(Date = test_split$Date) %>% group_split(Store, Dept)
# pre-allocate a list to store the predictions
test_pred <- vector(mode = "list", length = nrow(unique_pairs))
#train_pred <- vector(mode = "list", length = nrow(unique_pairs))
# perform regression for each split, note we used lm.fit instead of lm
for (i in 1:nrow(unique_pairs)) {
tmp_train <- train_split[[i]]
tmp_test <- test_split[[i]]
mycoef <- lm.fit(as.matrix(tmp_train[, -(2:4)]), tmp_train$Weekly_Sales)$coefficients
mycoef[is.na(mycoef)] <- 0
tmp_pred <- mycoef[1] + as.matrix(tmp_test[, 4:55]) %*% mycoef[-1]
#train_result <- mycoef[1] + as.matrix(tmp_train[, 4:55]) %*% mycoef[-1]
#train_pred[[i]] <- cbind(tmp_train[, 2:3], Date = tmp_train$Date, Weekly_Pred = train_result[,1])
test_pred[[i]] <- cbind(tmp_test[, 2:3], Date = tmp_test$Date, Weekly_Pred = tmp_pred[,1])
}
# turn the list into a table at once,
# this is much more efficient then keep concatenating small tables
test_pred <- bind_rows(test_pred)
View(test_pairs)
View(test_pred)
View(tmp_train)
View(tmp_train)
View(tmp_test)
View(tmp_train)
as.matrix(tmp_train[, -(2:4)]
print(as.matrix(tmp_train[, -(2:4)])
print(as.matrix(tmp_train[, -(2:4)]))
View(tmp_test)
View(tmp_test)
train_pairs <- train[, 1:2] %>% count(Store, Dept) %>% filter(n != 0)
test_pairs <- test[, 1:2] %>% count(Store, Dept) %>% filter(n != 0)
unique_pairs <- intersect(train_pairs[, 1:2], test_pairs[, 1:2])
# pick out the needed training samples, convert to dummy coding, then put them into a list
train_split <- unique_pairs %>%
left_join(train, by = c('Store', 'Dept')) %>%
mutate(Wk = factor(ifelse(lubridate::year(Date) == 2010, lubridate::week(Date) - 1, lubridate::week(Date)), levels = 1:52)) %>%
mutate(Yr = lubridate::year(Date))
train_split = as_tibble(model.matrix(~ Weekly_Sales + Store + Dept + Yr + Wk, train_split)) %>% group_split(Store, Dept)
# do the same for the test set
test_split <- unique_pairs %>%
left_join(test, by = c('Store', 'Dept')) %>%
mutate(Wk = factor(ifelse(lubridate::year(Date) == 2010, lubridate::week(Date) - 1, lubridate::week(Date)), levels = 1:52)) %>%
mutate(Yr = lubridate::year(Date))
test_split = as_tibble(model.matrix(~ Store + Dept + Yr + Wk, test_split)) %>% mutate(Date = test_split$Date) %>% group_split(Store, Dept)
# pre-allocate a list to store the predictions
test_pred <- vector(mode = "list", length = nrow(unique_pairs))
#train_pred <- vector(mode = "list", length = nrow(unique_pairs))
# perform regression for each split, note we used lm.fit instead of lm
for (i in 1:nrow(unique_pairs)) {
#tmp_train <- train_split[[i]]
#tmp_test <- test_split[[i]]
tmp_train <- as.data.frame(train_split[[i]])
tmp_test <- as.data.frame(test_split[[i]])
#mycoef <- lm.fit(as.matrix(tmp_train[, -(2:4)]), tmp_train$Weekly_Sales)$coefficients
mycoef <- lm.fit(as.matrix(tmp_train[, -2]), tmp_train$Weekly_Sales)$coefficients
#mycoef <- lm()
mycoef[is.na(mycoef)] <- 0
#tmp_pred <- mycoef[1] + as.matrix(tmp_test[, 4:55]) %*% mycoef[-1]
tmp_pred <- mycoef[1] + as.matrix(tmp_test[, 2:55]) %*% mycoef[-1]
#train_result <- mycoef[1] + as.matrix(tmp_train[, 4:55]) %*% mycoef[-1]
#train_pred[[i]] <- cbind(tmp_train[, 2:3], Date = tmp_train$Date, Weekly_Pred = train_result[,1])
test_pred[[i]] <- cbind(tmp_test[, 2:3], Date = tmp_test$Date, Weekly_Pred = tmp_pred[,1])
}
# turn the list into a table at once,
# this is much more efficient then keep concatenating small tables
test_pred <- bind_rows(test_pred)
source("mymain.R")
library(readr)
library(magrittr)
library(dplyr)
# read in train / test dataframes
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')
# wae: record weighted mean absolute error WMAE
num_folds <- 10
wae <- rep(0, num_folds)
for (t in 1:num_folds) {
# *** THIS IS YOUR PREDICTION FUNCTION ***
test_pred <- mypredict()
#print(test_pred)
# read new data from fold_t
fold_file <- paste0('fold_', t, '.csv')
new_train <- readr::read_csv(fold_file, col_types = cols())
# extract predictions matching up to the new data
scoring_tbl <- new_train %>% left_join(test_pred, by = c('Date', 'Store', 'Dept'))
#print(scoring_tbl)
# compute WMAE
actuals <- scoring_tbl$Weekly_Sales
preds <- scoring_tbl$Weekly_Pred
preds[is.na(preds)] <- 0
weights <- if_else(scoring_tbl$IsHoliday, 5, 1)
wae[t] <- sum(weights * abs(actuals - preds)) / sum(weights)
# update train data and get ready to predict at (t+1)
train <- train %>% add_row(new_train)
}
print(wae)
mean(wae)
source("mymain.R")
